{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don’t know how to make the cake. We need to solve the unsupervised learning problem before we can even think of getting to true AI.\n",
    "\n",
    "Yann LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights\n",
    "\n",
    "* the AI trains on the training data (experience) to improve its error rate (performance) in flagging spam (task), and the ultimate success criterion is how well its experience generalizes to new, never-before-seen data (generalization error).\n",
    "\n",
    "* We could also use machine learning systems as an exploration or data discovery tool to gain deeper insight into the problem we are trying to solve. For example, in the email spam filter example, we can learn which words or phrases are most predictive of spam and recognize newly emerging malicious spam patterns.\n",
    "\n",
    "* Because the problem does not have a strictly defined task, the AI agent may find interesting patterns above and beyond what we initially were looking for.\n",
    "\n",
    "* [...] unsupervised system is better than the supervised system at finding new patterns in future data[...]\n",
    "\n",
    "* Since the majority of the world’s data is unlabeled, with supervised learning, the ability of AI to expand its performance to never-before-seen instances is quite limited.\n",
    "\n",
    "* However, for problems where patterns are unknown or constantly changing or for which we do not have sufficiently large labeled datasets, unsupervised learning truly shines.\n",
    "\n",
    "\n",
    "* I think AI is akin to building a rocket ship. You need a huge engine and a lot of fuel. If you have a large engine and a tiny amount of fuel, you won’t make it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift off. To build a rocket you need a huge engine and a lot of fuel. - Andrew Ng\n",
    "\n",
    "\n",
    "* [...] we can introduce unsupervised learning as a regularizer. Regularization is a process used to reduce the complexity of a machine learning algorithm, helping it capture the signal in the data without adjusting too much to the noise. Unsupervised pretraining is one such form of regularization. Instead of feeding the original input data directly into a supervised learning algorithm, we can feed a new representation of the original input data that we generate.\n",
    "\n",
    "\n",
    "* Even with the advances in computational power, big data is hard for machine learning algorithms to manage. In general, adding more instances is not too problematic because we can parallelize operations using modern map-reduce solutions such as Spark,the more features we have, the more difficult training becomes.\n",
    "\n",
    "* When the features are very numerous, this search becomes very expensive, both from a time and compute perspective. [...] This problem is known as the curse of dimensionality, and unsupervised learning is well suited to help manage this.\n",
    "\n",
    "* With unsupervised learning, we can perform outlier detection using dimensionality reduction and create a solution specifically for the outliers and, separately, a solution for the normal data. \n",
    "\n",
    "* what we care about most is how well the machine learning solution generalizes to never-before-seen cases. The choice of the supervised learning algorithm is very important at minimizing this generalization error.\n",
    "\n",
    "<h3>Linear Regression</h3>\n",
    "* If the true relationship between the inputs and the output is linear and the input variables are not highly correlated (a situation known as collinearity), linear regression may be an appropriate choice. If the true relationship is more complex or nonlinear, linear regression will underfit the data.4\n",
    "\n",
    "<h3>Logistic Regression</h3>\n",
    "* When the classes we are trying to predict are nonoverlapping and linearly separable, logistic regression is an excellent choice.\n",
    "\n",
    "<h3> Neighborhood-Based Methods </h3>\n",
    "* Neighborhood-based models do not learn a set model to predict labels for new points; rather, these models predict labels for new points based purely on distance of new points to preexisting labeled points. \n",
    "\n",
    "<h3>K-Nearest Neighbors</h3>\n",
    "* To label each new point, KNN looks at a k number (where k is an integer value) of nearest labeled points and has these already labeled neighbors vote on how to label the new point.[...] KNN is very sensitive to the choice of k. When k is set too low, KNN can overfit, and when k is set too high, KNN can underfit.[...], [...] KNN can help predict what a user will like given what similar users like (known as collaborative filtering) or what the user has liked in the past (known as content-based filtering).\n",
    "\n",
    "<h3>Single Decision Tree </h3>\n",
    "* Single decision tree is usually poor at generalizing what it has learned during training to never-before-seen cases because it usually overfits the training data during its one and only training iteration.\n",
    "\n",
    "<h3> Bagging</h3>\n",
    "* To improve the single decision tree, we can introduce bootstrap aggregation (more commonly known as bagging), in which we take multiple random samples of instances from the training data, create a decision tree for each sample, and then predict the output for each instance by averaging the predictions of each of these trees. \n",
    "\n",
    "<h3> Boosting </h3>\n",
    "* Another approach, known as boosting, is used to create multiple trees like in bagging but to build the trees sequentially, using what the AI learned from the previous tree to improve results on the subsequent tree. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>UNSUPERVISED LEARNING </h1>\n",
    "<h2>Dimensionality Reduction</h2>\n",
    "* [...] projects the original high-dimensional input data to a low-dimensional space, filtering out the not-so-relevant features and keeping as much of the interesting ones as possible[...]\n",
    "\n",
    "<b> There are two major branches of dimensionality—linear projection and nonlinear dimensionality reduction.  </b>\n",
    "\n",
    "<h3> PCA </h3>\n",
    "* In PCA, the algorithm finds a low-dimensional representation of the data while retaining as much of the variation as possible [...]  We lose some of the variance by moving to this low-dimensional space, but the underlying structure of the data is easier to identify, allowing us to perform tasks like clustering more efficiently.\n",
    "\n",
    "<h3>SVD</h3>\n",
    "* Another approach to learning the underlying structure of the data is to reduce the rank of the original matrix of features to a smaller rank such that the original matrix can be recreated using a linear combination of some of the vectors in the smaller rank matrix. [...] To generate the smaller rank matrix, SVD keeps the vectors of the original matrix that have the most information (i.e., the highest singular value). The smaller rank matrix captures the most important elements of the original feature space.\n",
    "\n",
    "<h2> MANIFOLD LEARNING </h2>\n",
    "* [...] Instead of a linear projection, it may be better to perform a nonlinear transformation of the data—this is known as manifold learning or nonlinear dimensionality reduction.\n",
    "<h3>ISOMAP </h3>\n",
    "* This algorithm learns the intrinsic geometry of the data manifold by estimating the geodesic or curved distance between each point and its neighbors rather than the Euclidean distance. Isomap uses this to then embed the original high-dimensional space to a low-dimensional one.\n",
    "\n",
    "<h3>t-SNE</h3>\n",
    "\n",
    "* Embeds high-dimensional data into a space of just two or three dimensions, allowing the transformed data to be visualized. In this two- or three-dimensional space, similar instances are modeled closer together and dissimilar instances are modeled further away.\n",
    "\n",
    "<h3> Dictionary Learning </h3>\n",
    "* By creating such a dictionary, this algorithm is able to efficiently identify the most salient representative elements of the original feature space—these are the ones that have the most nonzero weights\n",
    "\n",
    "\n",
    "<h2>INDEPENDENT COMPONENT ANALYSISS</h2>\n",
    "* ICA is commonly used in signal processing tasks (for example, to identify the individual voices in an audio clip of a busy coffeehouse).\n",
    "\n",
    "\n",
    "<h2>LATENT DIRICHLET ALLOCATION</h2>\n",
    "* Unsupervised learning can also explain a dataset by learning why some parts of the dataset are similar to each other. This requires learning unobserved elements within the dataset—an approach known as latent Dirichlet allocation (LDA) [...] LDA is able to explain a given document with a small set of topics, where for each topic there is a small set of frequently used words. This is the hidden structure the LDA is able to capture, helping us better explain a previously unstructured corpus of text. \n",
    "\n",
    "\n",
    "\n",
    "<h1> CLUSTERING </h1>\n",
    "<h2> K-MEANS </h2>\n",
    "* To cluster well, we need to identify distinct groups such that the instances within a group are similar to each other but different from instances in other groups.\n",
    "<h2> HIERARCHICAL CLUSTERING </h2>\n",
    "* An alternative clustering approach—one that does not require us to precommit to a particular number of clusters [...]\n",
    "<h2>DBSCAN</h2>\n",
    "* If an instance is within this specified distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this specified distance of another cluster is labeled an outlier.\n",
    "\n",
    "<h2>AUTOENCODERS</h2>\n",
    "* To generate new feature representations, we can use a feedforward, nonrecurrent neural network to perform representation learning, where the number of nodes in the output layer matches the number of nodes in the input layer. This neural network is known as an autoencoder and effectively reconstructs the original features, learning a new representation using the hidden layers in between.\n",
    "\n",
    "<h2>FEATURE EXTRACTION USING SUPERVISED TRAINING OF FEEDFORWARD NETWORKS</h2>\n",
    "\n",
    "* If we have labels, an alternate feature extraction approach is to use a feedforward, nonrecurrent neural network where the output layer attempts to predict the correct label. Just like with autoencoders, each hidden layer learns a representation of the original features.\n",
    "\n",
    "\n",
    "<h2>Unsupervised Deep Learning</h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
